# .md에 적어놔도 AI가 안 읽는 문제, MCP 메모리 서버로 해결하기

## AI의 기억력 문제

Claude Code를 쓰면서 `CLAUDE.md`에 이런저런 지시를 적어둔다. "bun을 써라", "테스트는 항상 돌려라", "이 프로젝트는 이런 구조다" 같은 것들.

문제는, **AI가 이걸 매번 확인하지 않는다는 것이다.**

대화가 길어지면 `.md` 파일의 내용은 context 밖으로 밀려난다. 그러면 AI는 이전에 받은 지시를 무시하고, 결국 사용자가 "아까 말했잖아"라고 하면 그제야 "아 맞다요"라고 하는 상황이 반복된다.

설령 매번 파일을 읽더라도, 200줄짜리 `.md` 파일 전체를 context에 넣는 건 토큰 낭비다. 지금 대화와 관련 없는 내용이 대부분이니까.

이 두 가지 문제를 풀기 위해 **mcp-memory-server**를 만들었다.

---

## 핵심 아이디어

```
기존: .md 파일 전체를 context에 넣거나, 아예 안 읽음
개선: 매 턴 자동으로 관련 기억만 검색해서 주입
```

### 1. 매 턴 자동 검색

MCP 서버의 `auto_search` 도구가 매 대화 턴마다 실행된다. 사용자의 메시지를 벡터 임베딩으로 변환하고, 저장된 기억 중 의미적으로 관련 있는 것만 골라서 context에 넣는다.

AI가 "알아서 확인"하지 않는 문제가 해결된다. 과거에 "bun을 써라"라고 했으면, 패키지 관련 대화가 나올 때 자동으로 그 기억이 올라온다.

### 2. 토큰 효율적 주입

200줄 전체를 넣는 대신, **토큰 budget**(기본 1024) 내에서 가장 관련 있는 기억만 선별한다. 게다가 3가지 해상도로 압축한다:

- **Level 0**: 원문 그대로 ("나는 Python과 FastAPI를 주로 사용하고, bun을 패키지 매니저로 쓴다")
- **Level 1**: 키워드 요약 ("Python, FastAPI, bun 사용")
- **Level 2**: 엔티티 트리플 ("사용자,사용함,Python")

budget이 넉넉하면 원문을, 빡빡하면 트리플로 넣는다. 관련 기억 10개를 넣더라도 토큰을 적게 쓸 수 있다.

---

## RL이 저장을 판단한다

"무엇을 기억할지"가 사실 가장 중요하다.

모든 대화를 다 저장하면 노이즈가 쌓이고, 너무 적게 저장하면 기억이 없다. 이 균형을 **RL contextual bandit**이 잡는다.

```
사용자: "나는 매운 거 못 먹어"
→ importance: 개인정보(0.4) + 선호(0.35) = 0.75
→ 0.75 ≥ 0.7 → 즉시 SAVE

사용자: "ㅋㅋ 그래?"
→ importance: 0.0
→ 0.0 ≤ 0.1 → 즉시 SKIP

사용자: "요즘 Rust 공부 중인데 소유권이 어려워"
→ importance: 기술(0.3) + 감정(0.2) = 0.5
→ 0.1 < 0.5 < 0.7 → MLP bandit이 결정
```

고확신 영역(≥0.7, ≤0.1)은 rule-based로 빠르게 처리하고, 중간 영역만 RL에게 맡긴다. 안전하면서도 학습 가능한 구조다.

사용자가 "맞아", "좋아" 같은 긍정 피드백을 주면 RL이 강화되고, "아니야", "틀렸어"하면 보수적으로 돌아간다.

---

## 기억은 잊혀져야 한다

인간의 기억처럼, AI 기억도 시간이 지나면 중요도가 떨어진다.

**수면 사이클**이 주기적으로 실행되면서:

1. **통합**: 유사도 92% 이상인 중복 기억을 병합
2. **압축**: 오래되고 안 쓰이는 기억을 Level 2 트리플로 압축
3. **비활성화**: 더 오래되면 검색에서 제외
4. **삭제**: 30일 이상 비활성이면 영구 삭제

중요한 기억은 `pin`으로 보호할 수 있고, `immutable`로 절대 삭제 불가능하게도 만들 수 있다.

---

## 지식 그래프로 관계 추론

ChromaDB의 벡터 검색만으로는 한계가 있다.

"봉골레 파스타 추천해줘"라고 했을 때, "사용자가 조개를 싫어한다"는 기억이 벡터 유사도로는 안 잡힐 수 있다. 텍스트가 다르니까.

이걸 **Knowledge Graph**가 해결한다:

```
사용자 ──싫어함──→ 조개
봉골레 ──재료──→ 조개
```

"봉골레" → "조개" → "싫어함" 경로를 2-hop으로 탐색해서, 벡터 검색에서 놓친 관련 기억을 찾아낸다.

최종 점수는 벡터 유사도(60%)와 그래프 연결(40%)을 융합한다.

---

## 사용법

### 설치

```bash
pip install mcp-memory-server
```

### Claude Desktop 연결

`claude_desktop_config.json`에 추가:

```json
{
  "mcpServers": {
    "aimemory": {
      "command": "aimemory-mcp"
    }
  }
}
```

### Claude Code 연결

```bash
claude mcp add aimemory -- aimemory-mcp
```

이게 끝이다. 이후 Claude가 자동으로 12개 MCP 도구를 사용하며 기억을 관리한다.

---

## 기술 스택

- **Python 3.11+** — 타입 힌트 + Pydantic v2
- **ChromaDB** — 벡터 저장소 (SQLite 기반, 로컬 전용)
- **sentence-transformers** — 다국어 임베딩 (`intfloat/multilingual-e5-small`)
- **NetworkX** — 지식 그래프
- **FastMCP** — MCP 서버 프레임워크
- **590개 테스트** 통과, GitHub Actions CI

---

## 앞으로

- ChromaDB 외 다른 벡터 DB 지원 (Qdrant, Milvus)
- MCP 서버 디렉토리 등록 (awesome-mcp-servers PR 제출 완료)
- 웹 대시보드 (기억 시각화, 지식 그래프 탐색)
- 더 많은 언어 패턴 지원

**GitHub**: https://github.com/ihwooMil/mcp-memory-server
**PyPI**: https://pypi.org/project/mcp-memory-server/
**License**: MIT
