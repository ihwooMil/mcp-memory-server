# AI Memory System - 구현 계획서

> **기반 문서**: ai_memory_system_design_v4.docx
> **생성일**: 2026-02-25
> **목표**: RL Feature Extractor + Intelligent Memory MCP 시스템 구현

---

## 프로젝트 개요

현재 AI 메모리 도구의 두 가지 핵심 한계를 해결하는 시스템:
1. **수동 검색 문제** - 사용자가 명시적으로 요청해야만 메모리를 활용하는 문제 → 매 턴 자동 검색으로 해결
2. **토큰 낭비 문제** - 전체 기억을 매번 context에 삽입하는 문제 → top-K 선별 삽입으로 해결

### 시스템 아키텍처 (6 레이어)

| 레이어 | 역할 |
|--------|------|
| RL Feature Extractor | 대화에서 feature vector 생성 + 저장할 fact 결정 |
| Immutable Rule Filter | 불변 규칙 적용 (보안/프라이버시) |
| 저장소 | feature vector + fact/텍스트 저장 |
| 검색 | feature 유사도 매칭으로 관련 fact 조회 |
| Context Composer | 토큰 budget 내 최적 조합 |
| 수면 사이클 | 주기적 기억 정리/재인코딩/통합/망각 |

---

## Step 1: 학습 데이터 확보 (예상 2~3주)

> **목표**: RL Feature Extractor 학습에 필요한 데이터셋 확보
> **성공률**: ~100%
> **성공 기준**: reward 분포가 의미있게 분리됨 (positive/negative 골고루)

### 1.1 합성 데이터 생성 파이프라인

- [ ] 로컬 LLM 환경 세팅 (Qwen2.5-7B 또는 Llama-3.1-8B)
- [ ] Self-play 시뮬레이션 프레임워크 설계
  - [ ] 대화 시나리오 템플릿 정의 (일상대화, 기술질문, 프로젝트 관련 등)
  - [ ] 멀티턴 대화 생성 로직 구현
  - [ ] 기억 저장/활용 시점 시뮬레이션
- [ ] 로컬 모델로 1,000 에피소드 생성 실행
- [ ] 생성된 대화 품질 샘플링 검증 (수동 50개 검토)

### 1.2 Heuristic Reward 계산기

- [ ] Reward 함수 설계 및 구현
  - [ ] (+) 저장한 키워드/엔티티가 미래 턴에 재등장
  - [ ] (-) 사용자가 비슷한 질문을 반복 (기억 실패 신호)
  - [ ] (+) 저장 기억의 토큰 길이가 짧을수록 (효율 보너스)
- [ ] Self-play 결과에 대해 reward 자동 계산 테스트
- [ ] Reward 분포 시각화 및 분석

### 1.3 학습 데이터셋 구성

- [ ] (state, action, reward) 트리플 형식으로 데이터셋 구성
- [ ] 데이터셋 통계 확인
  - [ ] reward 분포
  - [ ] 평균 에피소드 길이
  - [ ] 액션 분포
- [ ] 학습/검증/테스트 셋 분리
- [ ] 데이터 저장 포맷 확정 (parquet / jsonl 등)

### 1.4 API 비용 최적화 검증

- [ ] 로컬 모델 대량 생성 파이프라인 동작 확인
- [ ] API는 품질 검증용 소량만 사용 ($10 이하 확인)
- [ ] 비용 로깅 시스템 구현

---

## Step 2: RL Feature Extractor (예상 2~4주)

> **목표**: 대화에서 feature vector를 추출하고 저장할 fact를 결정하는 RL 모델 학습
> **성공률**: 70~80%
> **성공 기준**: 테스트 대화 50개에서 feature 검색 precision/recall 측정

### 2.1 모델 아키텍처

- [ ] 입력 인코딩 설계
  - [ ] 로컬 LLM(7B) → last hidden state 추출 파이프라인
  - [ ] 텍스트 → embedding 변환 모듈
- [ ] RL 네트워크 구현
  - [ ] MLP 2~3층, ~5M params
  - [ ] State 정의: 현재 대화 맥락 + 기존 메모리 요약
  - [ ] Action 정의: 저장할 정보 선택 + 압축 수준 결정
  - [ ] Output: feature vector (64차원) + fact/텍스트
- [ ] 추론 속도 벤치마크 (~5ms 목표)

### 2.2 Offline 학습 파이프라인

- [ ] 학습 루프 구현 (PPO / SAC 등 알고리즘 선택)
- [ ] Step 1 데이터로 학습 실행
- [ ] Loss 감소 그래프 확인 및 기록
- [ ] Hyperparameter 튜닝
  - [ ] Learning rate
  - [ ] Batch size
  - [ ] Discount factor
- [ ] 학습 안정성 확인 (reward 수렴 확인)

### 2.3 Feature Vector 생성 + Fact 추출

- [ ] 대화 입력 → feature + fact 출력 파이프라인 구현
- [ ] Feature vector 품질 검증
  - [ ] 유사한 대화 → 유사한 feature 확인 (cosine similarity)
  - [ ] 다른 주제 대화 → 먼 feature 확인
- [ ] Fact 추출 품질 검증 (핵심 정보 누락 없는지)

### 2.4 DB 저장/검색 통합

- [ ] 저장소 선택 및 세팅 (SQLite + 벡터 인덱스 또는 벡터DB)
- [ ] 기억 단위 스키마 구현
  - [ ] feature vector (검색 인덱스)
  - [ ] structured fact (subject, predicate, object)
  - [ ] 다해상도 텍스트 (Level 0/1/2)
  - [ ] timestamp
  - [ ] 원본 대화 ID
- [ ] Feature 인덱스 + fact 저장 동작 확인
- [ ] Feature 매칭으로 fact 반환 확인

### 2.5 불변 규칙 필터 통합

- [ ] Level 1 하드코딩 규칙 구현
  - [ ] 비밀번호/API 키 저장 차단
  - [ ] 개인 의료정보 암호화 없이 저장 차단
  - [ ] 삭제 요청 시 즉시 완전 삭제
  - [ ] 원본 대화 ID 참조 필수
- [ ] API 키/비밀번호 포함 대화 → 저장 차단 테스트
- [ ] RL과 완전 분리 구조 확인

---

## Step 3: 망각 + 다해상도 저장 (예상 1~2주)

> **목표**: 기억 품질을 높이는 망각 시스템 + 토큰 효율을 높이는 다해상도 저장 구현
> **성공률**: 90%+
> **성공 기준**: 기억 1000개 상태에서 망각 전/후 검색 정밀도 비교 → 개선 확인

### 3.1 중요도 계산기

- [ ] access_count 추적 시스템 구현
- [ ] recency_decay 함수 구현 (지수 감쇠: e^(-λ × days))
- [ ] importance = access_count × recency_decay 계산 로직
- [ ] 기억 100개 대상으로 중요도 점수 산출 확인

### 3.2 망각 단계 구현

- [ ] **압축 단계**: importance < threshold_low
  - [ ] 다해상도 Level 0 → Level 2 강등 (entity triple만 유지)
  - [ ] 원본 대화 ID로 재구성 가능 확인
- [ ] **비활성화 단계**: importance < threshold_forget
  - [ ] 검색 인덱스에서 제거, 메타데이터만 유지
  - [ ] 메타데이터로 복원 가능 확인
- [ ] **영구 삭제 단계**: 비활성화 후 M일 경과
  - [ ] 완전 삭제 + audit log 기록
  - [ ] 복원 불가 확인
- [ ] 임계값 조정 → 각 단계 전이 테스트

### 3.3 망각 방지 메커니즘

- [ ] 사용자 핀 기능 구현 (중요 기억 망각 제외)
- [ ] 연결된 기억 보호 (참조 기반 중요도 보너스)
- [ ] 불변 규칙 준수 (삭제 시 audit log 필수)

### 3.4 다해상도 텍스트 시스템

- [ ] Level 0: 원문 그대로 저장
- [ ] Level 1: 요약 생성 (로컬 LLM 활용)
- [ ] Level 2: entity triple만 저장
- [ ] 같은 기억을 3가지 해상도로 저장 → 토큰 수 비교 테스트

### 3.5 토큰 Budget 기반 해상도 선택기

- [ ] Context Composer 구현
- [ ] Budget 1K/5K/10K 토큰 설정 → 적절한 해상도 자동 선택 확인
- [ ] Top-K 기억 선택 + 해상도 조합 최적화

---

## Step 4: 수면 사이클 + Progressive 확장 (예상 1~2주)

> **목표**: 주기적 배치 프로세스로 feature 재인코딩, 차원 확장, 기억 통합, 망각 통합 실행
> **성공률**: 90%+
> **성공 기준**: 수면 사이클 1회 실행 후 성능 저하 없음 + 검색 정밀도 유지/향상

### 4.1 수면 사이클 스케줄러

- [ ] 배치 스크립트/cron 스케줄러 구현
- [ ] 수동 트리거 지원
- [ ] 실행 로그 및 리포트 생성

### 4.2 필수 작업 구현

- [ ] **Feature 재인코딩**
  - [ ] 0 패딩된 기억의 원본 fact/텍스트를 현재 RL encoder로 재추출
  - [ ] 수면 사이클 후 0패딩 feature 수 = 0 확인
- [ ] **차원 확장 판단**
  - [ ] 기억 수 기준 확장 트리거 (~2K→64→128, ~10K→128→256)
  - [ ] 검색 precision 임계값 기반 트리거
- [ ] **Progressive 차원 확장 (64→128)**
  - [ ] 기존 feature zero-padding 구현
  - [ ] 0패딩 상태에서도 검색 동작 확인
  - [ ] 재인코딩 후 정밀도 향상 확인
- [ ] **기억 통합 (Consolidation)**
  - [ ] 중복 기억 merge 로직
  - [ ] Diff 저장 적용
  - [ ] 중복 기억 5쌍 입력 → 통합 후 건수 감소 확인
- [ ] **망각 처리 통합**
  - [ ] Step 3의 망각 시스템을 수면 사이클에 통합

### 4.3 선택 작업

- [ ] 다해상도 텍스트 재생성
- [ ] RL 모델 checkpoint 저장 (복원점 확보)

### 4.4 Online 학습 통합 (선택)

- [ ] 실시간 reward 수집 파이프라인
- [ ] Experience Replay Buffer 구현
- [ ] Catastrophic Forgetting 방지 (EWC 또는 Replay)
- [ ] Delayed Reward 처리 (주기적 활용도 평가 loop)
- [ ] Policy update → 추출 품질 변화 측정

---

## Step 5: RL Re-ranker (예상 1~2주, 선택)

> **목표**: DB 검색 결과를 사용자 패턴에 맞게 재정렬하는 경량 RL
> **성공률**: 80%+
> **성공 기준**: re-rank 후 응답 품질 측정 가능하게 개선, 레이턴시 증가 30ms 이하

### 5.1 Re-ranker 모델

- [ ] 경량 RL Re-ranker 설계 및 구현
- [ ] top-10 검색 결과 → re-rank → top-3 선택 파이프라인
- [ ] 학습 데이터 수집 (사용자 검색 패턴 기반)
- [ ] Online 학습 지원

### 5.2 성능 검증

- [ ] 레이턴시 측정: DB 검색 ~10ms + re-ranker ≤ 20ms (총 ≤ 30ms)
- [ ] A/B 비교: re-ranker 적용 전/후 응답 품질 비교
- [ ] re-rank 전/후 검색 결과 적절성 평가

---

## Step 6: P2P Federated Learning (예상 3~4주, 선택)

> **목표**: 여러 사용자의 학습 결과를 P2P로 공유하여 개별 학습 속도 향상
> **성공률**: 60~70%
> **성공 기준**: 노드 2개 간 gradient 공유 → 개별 학습 대비 수렴 속도 향상

### 6.1 모델 직렬화

- [ ] RL 모델 serialize/deserialize 구현
- [ ] 모델 저장 → 다른 인스턴스 로드 → 동일 추론 결과 확인

### 6.2 P2P 네트워크

- [ ] Gradient 교환 프로토콜 구현
- [ ] 노드 2개 간 gradient 송수신 성공 확인
- [ ] 글로벌/로컬 레이어 분리
  - [ ] 글로벌 공유 레이어: 일반적 기억 패턴
  - [ ] 로컬 개인화 레이어: 사용자 특수 패턴
- [ ] 글로벌 업데이트 후 로컬 개인화 유지 확인

### 6.3 보안

- [ ] Differential Privacy 적용 (gradient 노이즈)
- [ ] Byzantine-resilient Aggregation (중앙값 기반 집계)
- [ ] Level 3 규칙 검증 (불변 규칙 해시 교차 검증)
- [ ] 규칙 변조 노드 → 네트워크 접속 거부 테스트

### 6.4 성능 검증

- [ ] 개별 학습 vs P2P 학습 수렴 속도 비교
- [ ] 프라이버시 검증: gradient에서 원본 데이터 역추적 불가 테스트

---

## Step 7: MCP 서버 패키징 및 출시 (예상 1주)

> **목표**: 개발 완료된 시스템을 MCP 서버로 패키징하여 배포
> **성공 기준**: 10회 연속 대화에서 이전 대화의 기억이 자동으로 활용, 사용자 개입 없이 동작

### 7.1 MCP 서버 구현

- [ ] MCP 서버 코드 작성 (tool 정의 + 서버 실행)
- [ ] MCP Inspector에서 모든 tool 호출 성공 확인
- [ ] 자동 검색 미들웨어 구현 (매 턴 관련 기억 자동 삽입)

### 7.2 통합 테스트

- [ ] 사용자가 기억 검색 요청 없이도 관련 기억이 응답에 반영되는지 확인
- [ ] End-to-end 시나리오 테스트 (10회 연속 대화)
- [ ] 수면 사이클 자동화 (cron 설정) → 24시간 내 자동 실행 확인

### 7.3 배포 준비

- [ ] 설치 가이드 + README 작성
- [ ] README 따라 설치 → Claude Desktop 정상 동작 확인
- [ ] 의존성 정리 및 패키지 매니저 설정
- [ ] 에러 핸들링 및 로깅 최종 점검

---

## 전체 로드맵 요약

| Step | 내용 | 기간 | 성공률 | 상태 |
|------|------|------|--------|------|
| 1 | 학습 데이터 확보 | 2~3주 | ~100% | ⬜ 미착수 |
| 2 | RL Feature Extractor | 2~4주 | 70~80% | ⬜ 미착수 |
| 3 | 망각 + 다해상도 저장 | 1~2주 | 90%+ | ⬜ 미착수 |
| 4 | 수면 사이클 + Progressive 확장 | 1~2주 | 90%+ | ⬜ 미착수 |
| 5 | RL Re-ranker (선택) | 1~2주 | 80%+ | ⬜ 미착수 |
| 6 | P2P Federated Learning (선택) | 3~4주 | 60~70% | ⬜ 미착수 |
| 7 | MCP 서버 패키징 및 출시 | 1주 | ~100% | ⬜ 미착수 |

> **총 예상 기간**: 필수(Step 1~4 + 7) = 7~12주 / 전체(선택 포함) = 11~18주

---

## 핵심 원칙

- 각 Step은 이전 Step의 결과물을 기반으로 하며, 독립적으로 검증 가능
- **"저장 품질 > 검색 알고리즘"** 이 시스템의 기본 철학
- RL 모델을 serialize 가능하게 설계 → P2P 확장 용이
- MCP 서버는 최종 배포 형태 → 모든 기능 개발 완료 후 패키징
- Feature ≠ Fact: feature vector는 검색 키로만 사용, 저장되는 원본은 fact/텍스트
